{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script is designed to automate the process of creating tables \n",
    "in a PostgreSQL database and populating them with data from CSV files using Python libraries. \n",
    "It iterates through tables, creates the corresponding table on PostgreSQL and then populates them\n",
    "'''\n",
    "\n",
    "# Importing the necessary libraries\n",
    "# Numpy for data manipulation (just in case) \n",
    "# Pandas for reading CSV into a DF and analyzing it\n",
    "# Psycopg2 and sqlalchemy to interact with PostgreSQL database\n",
    "# OS to interact with the, well, OS and to analyze/manipulate files\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2 as pg2\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c0d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of dataframes from the .csv files\n",
    "df_dict = {file.split(\".csv\")[0]:pd.read_csv(file) for file in os.listdir() if file.endswith(\".csv\")}\n",
    "\n",
    "# Configuring pg2\n",
    "conn_pg2 = pg2.connect(database='BrazilEcom', user='postgres',password='****')\n",
    "\n",
    "#Connecting with the DB using sql_alch\n",
    "conn_string = 'postgresql://postgres:****@localhost/BrazilEcom'\n",
    "db = create_engine(conn_string)\n",
    "conn_sql_alch = db.connect()\n",
    "#FINALLY SQLALCHEMY WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1895901",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in df_dict:\n",
    "    print(file)\n",
    "    df_dict[file].info()\n",
    "    print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a8afd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing all the .csv files in the current dir\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".csv\"):\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e347f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many null rows we have in each table\n",
    "print(\"Total number of NaN rows: \\n\")\n",
    "for file in df_dict:\n",
    "    print(f\"{file} : {df_dict[file].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll create a table in PostgreSQL for each DataFrame and then load it\n",
    "\n",
    "command = '''\n",
    "CREATE TABLE IF NOT EXISTS olist_customers_dataset (\n",
    "index INTEGER,\n",
    "customer_id VARCHAR(50) PRIMARY KEY,\n",
    "customer_unique_id VARCHAR(50),\n",
    "customer_zip_code_prefix INTEGER,\n",
    "customer_city VARCHAR(50),\n",
    "customer_state VARCHAR(50)\n",
    ")\n",
    "'''\n",
    "cur = conn_pg2.cursor()\n",
    "cur.execute(command)\n",
    "cur.close()\n",
    "conn_pg2.commit()\n",
    "print(\"Successfully Created Table in SQL\")\n",
    "\n",
    "# !!IMPORTANT!!\n",
    "\n",
    "# we \"APPEND\" the data into the table because it already exists (and has 0 entries)\n",
    "# Have we set if_exists to \"REPLACE\", it would Drop the table before inserting new values.\n",
    "# If the REPLACE option is selected, the PK CONSTRAINT that we've set before dissapears\n",
    "# Check out https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html for more info\n",
    "\n",
    "# !!IMPORTANT!!\n",
    "\n",
    "# Here we set index True since we don't originally have a PK and we want to create it while loading the data\n",
    "# We want the index to start from 1, so we change the df index\n",
    "df_dict[\"olist_customers_dataset\"].index += 1\n",
    "# We also added this while creating the table so the .to_sql can find that column when appending data\n",
    "df_dict[\"olist_customers_dataset\"].to_sql(\"olist_customers_dataset\", con=conn_sql_alch, if_exists='append', index=True)\n",
    "conn_sql_alch.autocommit = True\n",
    "print(\"Successfully Uploaded values in SQL\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47bed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "command = '''\n",
    "CREATE TABLE IF NOT EXISTS olist_geolocation_dataset (\n",
    "index INTEGER,\n",
    "geolocation_zip_code_prefix INTEGER,\n",
    "geolocation_lat DOUBLE PRECISION,\n",
    "geolocation_lng DOUBLE PRECISION,\n",
    "geolocation_city VARCHAR(50),\n",
    "geolocation_state VARCHAR(50)\n",
    ")\n",
    "''' \n",
    "cur = conn_pg2.cursor()\n",
    "cur.execute(command)\n",
    "cur.close()\n",
    "conn_pg2.commit()\n",
    "print(\"Successfully Created Table in SQL\")\n",
    "\n",
    "# Here we set index True since we don't originally have a PK and we want to create it while loading the data\n",
    "# We want the index to start from 1, so we change the df index\n",
    "df_dict[\"olist_geolocation_dataset\"].index += 1\n",
    "# We also added this while creating the table so the .to_sql can find that column when appending data\n",
    "df_dict[\"olist_geolocation_dataset\"].to_sql(\"olist_geolocation_dataset\", con=conn_sql_alch, if_exists='append', index=True)\n",
    "conn_sql_alch.autocommit = True\n",
    "print(\"Successfully Uploaded values in SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '''\n",
    "CREATE TABLE IF NOT EXISTS olist_orders_dataset (\n",
    "index INTEGER,\n",
    "order_id VARCHAR(50) PRIMARY KEY,\n",
    "customer_id VARCHAR(50),\n",
    "order_status VARCHAR(50),\n",
    "order_purchase_timestamp TIMESTAMP,\n",
    "order_approved_at TIMESTAMP,\n",
    "order_delivered_carrier_date TIMESTAMP,\n",
    "order_delivered_customer_date TIMESTAMP,\n",
    "order_estimated_delivery_date TIMESTAMP\n",
    ")\n",
    "''' \n",
    "cur = conn_pg2.cursor()\n",
    "cur.execute(command)\n",
    "cur.close()\n",
    "conn_pg2.commit()\n",
    "print(\"Successfully Created Table in SQL\")\n",
    "\n",
    "\n",
    "df_dict[\"olist_orders_dataset\"].index += 1\n",
    "df_dict[\"olist_orders_dataset\"].to_sql(\"olist_orders_dataset\", con=conn_sql_alch, if_exists='append', index=True)\n",
    "conn_sql_alch.autocommit = True\n",
    "print(\"Successfully Uploaded values in SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '''\n",
    "CREATE TABLE IF NOT EXISTS olist_order_items_dataset (\n",
    "index INTEGER,\n",
    "order_id VARCHAR(50),\n",
    "order_item_id SMALLINT,\n",
    "product_id VARCHAR(50),\n",
    "seller_id VARCHAR(50),\n",
    "shipping_limit_date TIMESTAMP,\n",
    "price MONEY,\n",
    "freight_value DOUBLE PRECISION\n",
    ")\n",
    "''' \n",
    "cur = conn_pg2.cursor()\n",
    "cur.execute(command)\n",
    "cur.close()\n",
    "conn_pg2.commit()\n",
    "print(\"Successfully Created Table in SQL\")\n",
    "\n",
    "\n",
    "df_dict[\"olist_order_items_dataset\"].index += 1\n",
    "df_dict[\"olist_order_items_dataset\"].to_sql(\"olist_order_items_dataset\", con=conn_sql_alch, if_exists='append', index=True)\n",
    "conn_sql_alch.autocommit = True\n",
    "print(\"Successfully Uploaded values in SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '''\n",
    "CREATE TABLE IF NOT EXISTS olist_order_payments_dataset (\n",
    "index INTEGER,\n",
    "order_id VARCHAR(50),\n",
    "payment_sequential SMALLINT,\n",
    "payment_type VARCHAR(50),\n",
    "payment_installments SMALLINT,\n",
    "payment_value MONEY\n",
    ")\n",
    "'''\n",
    "cur = conn_pg2.cursor()\n",
    "cur.execute(command)\n",
    "cur.close()\n",
    "conn_pg2.commit()\n",
    "print(\"Successfully Created Table in SQL\")\n",
    "\n",
    "\n",
    "\n",
    "df_dict[\"olist_order_payments_dataset\"].index += 1\n",
    "df_dict[\"olist_order_payments_dataset\"].to_sql(\"olist_order_payments_dataset\", con=conn_sql_alch, if_exists='append', index=True)\n",
    "conn_sql_alch.autocommit = True\n",
    "print(\"Successfully Uploaded values in SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca6b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '''\n",
    "CREATE TABLE IF NOT EXISTS olist_order_reviews_dataset (\n",
    "index INTEGER,\n",
    "review_id VARCHAR(50),\n",
    "order_id VARCHAR(50),\n",
    "review_score SMALLINT,\n",
    "review_comment_title text,\n",
    "review_comment_message text,\n",
    "review_creation_date TIMESTAMP,\n",
    "review_answer_timestamp TIMESTAMP\n",
    ")\n",
    "'''\n",
    "cur = conn_pg2.cursor()\n",
    "cur.execute(command)\n",
    "cur.close()\n",
    "conn_pg2.commit()\n",
    "print(\"Successfully Created Table in SQL\")\n",
    "\n",
    "\n",
    "\n",
    "df_dict[\"olist_order_reviews_dataset\"].index += 1\n",
    "df_dict[\"olist_order_reviews_dataset\"].to_sql(\"olist_order_reviews_dataset\", con=conn_sql_alch, if_exists='append', index=True)\n",
    "conn_sql_alch.autocommit = True\n",
    "print(\"Successfully Uploaded values in SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec1727",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '''\n",
    "CREATE TABLE IF NOT EXISTS olist_products_dataset (\n",
    "index INTEGER,\n",
    "product_id VARCHAR(50) PRIMARY KEY,\n",
    "product_category_name VARCHAR(50),\n",
    "product_name_lenght SMALLINT,\n",
    "product_description_lenght INTEGER,\n",
    "product_photos_qty SMALLINT,\n",
    "product_weight_g INTEGER,\n",
    "product_length_cm SMALLINT,\n",
    "product_height_cm SMALLINT,\n",
    "product_width_cm SMALLINT\n",
    ")\n",
    "''' \n",
    "cur = conn_pg2.cursor()\n",
    "cur.execute(command)\n",
    "cur.close()\n",
    "conn_pg2.commit()\n",
    "print(\"Successfully Created Table in SQL\")\n",
    "\n",
    "\n",
    "\n",
    "df_dict[\"olist_products_dataset\"].index += 1\n",
    "df_dict[\"olist_products_dataset\"].to_sql(\"olist_products_dataset\", con=conn_sql_alch, if_exists='append', index=True)\n",
    "conn_sql_alch.autocommit = True\n",
    "print(\"Successfully Uploaded values in SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '''\n",
    "CREATE TABLE IF NOT EXISTS olist_sellers_dataset (\n",
    "index INTEGER,\n",
    "seller_id VARCHAR(50) PRIMARY KEY,\n",
    "seller_zip_code_prefix INTEGER,\n",
    "seller_city VARCHAR(50),\n",
    "seller_state VARCHAR(50)\n",
    ")\n",
    "''' \n",
    "cur = conn_pg2.cursor()\n",
    "cur.execute(command)\n",
    "cur.close()\n",
    "conn_pg2.commit()\n",
    "print(\"Successfully Created Table in SQL\")\n",
    "\n",
    "\n",
    "df_dict[\"olist_sellers_dataset\"].index += 1\n",
    "df_dict[\"olist_sellers_dataset\"].to_sql(\"olist_sellers_dataset\", con=conn_sql_alch, if_exists='append', index=True)\n",
    "conn_sql_alch.autocommit = True\n",
    "print(\"Successfully Uploaded values in SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '''\n",
    "CREATE TABLE IF NOT EXISTS product_category_name_translation (\n",
    "index INTEGER,\n",
    "product_category_name VARCHAR(50) PRIMARY KEY,\n",
    "product_category_name_english VARCHAR(50)\n",
    ")\n",
    "''' \n",
    "cur = conn_pg2.cursor()\n",
    "cur.execute(command)\n",
    "cur.close()\n",
    "conn_pg2.commit()\n",
    "print(\"Successfully Created Table in SQL\")\n",
    "\n",
    "\n",
    "\n",
    "df_dict[\"product_category_name_translation\"].index += 1\n",
    "df_dict[\"product_category_name_translation\"].to_sql(\"product_category_name_translation\", con=conn_sql_alch, if_exists='append', index=True)\n",
    "conn_sql_alch.autocommit = True\n",
    "print(\"Successfully Uploaded values in SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7107699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
